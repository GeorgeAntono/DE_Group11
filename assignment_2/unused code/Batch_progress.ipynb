{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdbea02-933c-4543-80b3-864891f4ecba",
   "metadata": {},
   "source": [
    "## Alles good doen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9c8397-b499-48f6-a611-69cd68cc5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"BigqueryExample\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "# Load data from BigQuery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846bddff-05ee-4b20-b9c7-bc56043c2f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- has_spouse: boolean (nullable = true)\n",
      " |-- spouse_ID: long (nullable = true)\n",
      " |-- gross_salary: long (nullable = true)\n",
      " |-- has_student_loan: boolean (nullable = true)\n",
      " |-- student_loan_amount: long (nullable = true)\n",
      " |-- has_general_loan: boolean (nullable = true)\n",
      " |-- general_loan_amount: long (nullable = true)\n",
      " |-- has_alimony: boolean (nullable = true)\n",
      " |-- alimony_amount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_individuals = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .load(\" degroup11.group11dataset.individuals\")    # project_id.datatset.tablename. Use your project id\n",
    "df_individuals.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f6e66a-068c-4c48-9ba7-7f35a679d7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- spouse_ID: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gross_salary: long (nullable = true)\n",
      " |-- has_student_loan: boolean (nullable = true)\n",
      " |-- student_loan_amount: long (nullable = true)\n",
      " |-- has_general_loan: boolean (nullable = true)\n",
      " |-- general_loan_amount: long (nullable = true)\n",
      " |-- has_alimony: boolean (nullable = true)\n",
      " |-- alimony_amount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spouse = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .load(\" degroup11.group11dataset.spouse\")   \n",
    "df_spouse.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3be8232-467c-486e-b459-9ef55cd84a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|Toetsinkomen|Toetsrente|\n",
      "+------------+----------+\n",
      "|           0|      17.5|\n",
      "|       19500|      17.5|\n",
      "|       20000|      18.5|\n",
      "|       20050|      19.5|\n",
      "|       21000|      20.5|\n",
      "|       21500|      21.0|\n",
      "|       22000|      22.0|\n",
      "|       22500|      23.5|\n",
      "|       23000|      24.5|\n",
      "|       23500|      25.0|\n",
      "|       24000|      25.5|\n",
      "|       25000|      26.5|\n",
      "|       26000|      27.0|\n",
      "|       28000|      27.5|\n",
      "|       55000|      28.0|\n",
      "|       58000|      28.5|\n",
      "|       61000|      29.0|\n",
      "|       63000|      29.5|\n",
      "|       65000|      30.0|\n",
      "|       68000|      30.5|\n",
      "+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_toetsinkomen = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .load(\" degroup11.group11dataset.toetsinkomen\")   \n",
    "df_toetsinkomen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a000fb54-77a2-4770-932f-165799761d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "df_comb = df_individuals.alias(\"ind\").join(df_spouse.alias(\"sp\"), col(\"ind.ID\") ==  col(\"sp.spouse_ID\"),\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189ea9e2-e351-442f-93b2-1195be4b48d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+----------+------------+--------------+----------------------+\n",
      "|gross_salary|alimony_amount|has_spouse|gross_salary|alimony_amount|house_spendable_income|\n",
      "+------------+--------------+----------+------------+--------------+----------------------+\n",
      "|       75526|             0|      true|       59122|             0|                 95233|\n",
      "|      134600|          1200|     false|      101412|             0|                120200|\n",
      "|      144798|             0|      true|      149733|             0|                194709|\n",
      "|      150580|             0|      true|       47582|             0|                166440|\n",
      "|      186763|             0|     false|      181201|             0|                186763|\n",
      "|       59116|             0|      true|       38986|             0|                 72111|\n",
      "|      189883|          3697|      true|      143379|             0|                193312|\n",
      "|      157499|             0|      true|       65001|             0|                179166|\n",
      "|      171667|          1858|      true|      136836|             0|                194983|\n",
      "|      168304|             0|     false|       60684|             0|                168304|\n",
      "|       75892|             0|      true|       88871|             0|                105515|\n",
      "|      160809|          2390|      true|       80447|             0|                158944|\n",
      "|       97033|             0|     false|      109320|             0|                 97033|\n",
      "|       94589|             0|      true|      108912|             0|                130893|\n",
      "|       49877|             0|     false|       61783|             0|                 49877|\n",
      "|      106209|             0|     false|       26954|             0|                106209|\n",
      "|       77328|             0|      true|      181158|             0|                137714|\n",
      "|      195138|             0|      true|       50156|             0|                211856|\n",
      "|       47206|             0|     false|       43192|             0|                 47206|\n",
      "|      102180|             0|     false|      174920|             0|                102180|\n",
      "+------------+--------------+----------+------------+--------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_comb\n",
    "# Convert necessary columns to appropriate data types (e.g., from string to integer/boolean)\n",
    "df = df.withColumn(\"ind.gross_salary\", col(\"ind.gross_salary\").cast(\"int\"))\n",
    "df = df.withColumn(\"ind.alimony_amount\", col(\"ind.alimony_amount\").cast(\"int\"))\n",
    "df = df.withColumn(\"ind.has_spouse\", col(\"ind.has_spouse\") == \"True\")\n",
    "df = df.withColumn(\"sp.gross_salary\", col(\"sp.gross_salary\").cast(\"int\"))\n",
    "df = df.withColumn(\"sp.alimony_amount\", col(\"sp.alimony_amount\").cast(\"int\"))\n",
    "\n",
    "# Apply the formula\n",
    "df = df.withColumn(\"house_spendable_income\", \n",
    "                   when(col(\"ind.has_spouse\") == False, col(\"ind.gross_salary\") - 12 * col(\"ind.alimony_amount\"))\n",
    "                   .otherwise(col(\"ind.gross_salary\") - 12 * col(\"ind.alimony_amount\")+ 1/3 * (col(\"sp.gross_salary\")-12 * col(\"sp.alimony_amount\"))))\n",
    "\n",
    "df = df.withColumn(\"house_spendable_income\", col(\"house_spendable_income\").cast(\"int\"))\n",
    "#df_comb.select(\"ind.student_loan_amount\", \"sp.student_loan_amount\").show()\n",
    "df.select(\"ind.gross_salary\",\"ind.alimony_amount\",\"ind.has_spouse\",\"sp.gross_salary\",\"sp.alimony_amount\", \"house_spendable_income\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a7848-895c-4b8b-ba06-8c1e1b9718c9",
   "metadata": {},
   "source": [
    "## Step 2 get for each row the corresponding toetsinkomen rente and add as column to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a0efdd3-89d8-40d9-9a2c-b557d6aa3f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 466, in __getnewargs__\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/serializers.py:459\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:466\u001b[0m, in \u001b[0;36mSparkContext.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getnewargs__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    467\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTEXT_ONLY_VALID_ON_DRIVER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    468\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    469\u001b[0m     )\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m custom_udf \u001b[38;5;241m=\u001b[39m udf(get_interest, FloatType())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Apply the mapping function using withColumn to create a new column \"Toetsrente\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m df_income_w_interest \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToetsrente\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m---> 15\u001b[0m     \u001b[43mcustom_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhouse_spendable_income\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m df_income_w_interest\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:425\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:402\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    400\u001b[0m         sc\u001b[38;5;241m.\u001b[39mprofiler_collector\u001b[38;5;241m.\u001b[39madd_profiler(\u001b[38;5;28mid\u001b[39m, memory_profiler)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_judf\u001b[49m\n\u001b[1;32m    403\u001b[0m     jPythonUDF \u001b[38;5;241m=\u001b[39m judf\u001b[38;5;241m.\u001b[39mapply(_to_seq(sc, cols, _to_java_column))\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jPythonUDF)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:322\u001b[0m, in \u001b[0;36mUserDefinedFunction._judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_judf\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# It is possible that concurrent access, to newly created UDF,\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# will initialize multiple UserDefinedPythonFunctions.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# This is unlikely, doesn't affect correctness,\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# and should have a minimal performance impact.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_judf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:331\u001b[0m, in \u001b[0;36mUserDefinedFunction._create_judf\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    328\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_getActiveSessionOrCreate()\n\u001b[1;32m    329\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[0;32m--> 331\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m jdt \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mparseDataType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnType\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:60\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     command \u001b[38;5;241m=\u001b[39m (func, returnType)\n\u001b[0;32m---> 60\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m     64\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m     70\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:5251\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   5249\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   5250\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 5251\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   5254\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/serializers.py:469\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    468\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "def get_interest(inkomen):\n",
    "    df = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .load(\" degroup11.group11dataset.toetsinkomen\") \n",
    "    rente = df.filter(df['Toetsinkomen'] < inkomen).orderBy(df['Toetsinkomen'].desc()).first()\n",
    "    return rente['Toetsrente']\n",
    "\n",
    "custom_udf = udf(get_interest, FloatType())\n",
    "\n",
    "# Apply the mapping function using withColumn to create a new column \"Toetsrente\"\n",
    "df_income_w_interest = df.withColumn(\n",
    "    \"Toetsrente\", \n",
    "    custom_udf(col(\"house_spendable_income\"))\n",
    ")\n",
    "df_income_w_interest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83dfc9ee-2bbc-44d8-8881-8be00cd7ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a618d-82a5-4ffb-ae9e-2f678ee27d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
