{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col,lit, rand\n",
    "\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"DataSourceSinkExample\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "#  Google Storage File Path\n",
    "gsc_file_path = 'gs://data_degroup11/house_pricing.csv'  #  use your gcp bucket name. Also upload sales.csv first\n",
    "gsc_file_path_2 = 'gs://data_degroup11/individuals.csv' \n",
    "gsc_file_path_3 = 'gs://data_degroup11/spouse.csv'\n",
    "# Create data frame\n",
    "df_house = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(gsc_file_path)\n",
    "df_house.printSchema()\n",
    "\n",
    "df_ind= spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(gsc_file_path_2)\n",
    "df_ind.printSchema()\n",
    "\n",
    "df_spouse = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(gsc_file_path_3)\n",
    "df_spouse.printSchema()\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(\"/home/jovyan/data/house_pricing.csv\")\n",
    "df.printSchema()\n",
    "\n",
    "df_ind = df_ind.na.fill({'has_alimony': False})\n",
    "df_spouse = df_spouse.na.fill({'has_alimony': False})\n",
    "\n",
    "newDf_ind = df_ind.select('has_alimony')  # select one column\n",
    "\n",
    "newDf_ind.show()\n",
    "\n",
    "newDf_spouse = df_spouse.select('has_alimony')  # select one column\n",
    "\n",
    "newDf_spouse.show()\n",
    "\n",
    "# Rename the 'ID' column to 'spouse_ID' in df_spouse\n",
    "df_spouse = df_spouse.withColumnRenamed('ID', 'spouse_ID')\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_spouse.show()\n",
    "\n",
    "# Add a new column 'spouse_id' with the same values as 'ID'\n",
    "df_ind = df_ind.withColumn('spouse_id', col('ID'))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_ind.show()\n",
    "\n",
    "# Print the column names of df_ind\n",
    "print(\"Column Names of df_ind:\")\n",
    "print(df_ind.columns)\n",
    "\n",
    "# Reorder the columns based on the desired configuration\n",
    "new_columns = ['ID', 'age', 'has_spouse', 'spouse_ID', 'gross_salary', 'has_student_loan', 'student_loan_amount', 'has_general_loan', 'general_loan_amount', 'has_alimony', 'alimony_amount']\n",
    "df_ind = df_ind.select(*new_columns)\n",
    "\n",
    "# Add a new column 'Availability' with random True or False values\n",
    "df_house = df_house.withColumn('Availability', (rand() < lit(0.5)))\n",
    "\n",
    "newDf_house = df_house.select('Availability')  # select one column\n",
    "\n",
    "newDf_house.show()\n",
    "\n",
    "# Specify the GCS path where you want to save the new DataFrames\n",
    "output_path_ind = 'gs://data_degroup11/individuals_updated2.csv'\n",
    "output_path_spouse = 'gs://data_degroup11/spouse_updated2.csv'\n",
    "output_path_house = 'gs://data_degroup11/house_pricing_updated2.csv'\n",
    "\n",
    "output_path_ind_test = 'gs://data_degroup11/individuals_updated_test.csv'\n",
    "output_path_spouse_test = 'gs://data_degroup11/spouse_updated_test.csv'\n",
    "\n",
    "\n",
    "# Write each partition to a separate CSV file on GCS\n",
    "df_ind.write.partitionBy('ID').format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_path_ind_test)\n",
    "df_spouse.write.partitionBy('spouse_ID').format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_path_spouse_test)\n",
    "\n",
    "# Write the new DataFrames to GCS\n",
    "df_ind.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_path_ind)\n",
    "df_spouse.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_path_spouse)\n",
    "df_house.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_path_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3009a1a6d8c73d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#hadoop fs -getmerge -nl gs://data_degroup11/individuals_updated_test/ gs://data_degroup11/individuals_updated_test.csv\n",
    "#hadoop fs -getmerge -nl gs://data_degroup11/spouse_updated_test/ gs://data_degroup11/spouse_updated_test/spouse_updated_test.csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25bf990b065012f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
