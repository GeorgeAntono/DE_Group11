{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e16d45c-93eb-4953-b6b3-ee867322cf03",
   "metadata": {},
   "source": [
    "## Starting streaming pipeline with Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d9285b-8113-4943-9029-01f618dffa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_json(value): struct (nullable = true)\n",
      " |    |-- Address: string (nullable = true)\n",
      " |    |-- City: string (nullable = true)\n",
      " |    |-- Price: integer (nullable = true)\n",
      " |    |-- Lot_size: string (nullable = true)\n",
      " |    |-- Living_space_size: string (nullable = true)\n",
      " |    |-- Build_year: string (nullable = true)\n",
      " |    |-- Build_type: string (nullable = true)\n",
      " |    |-- House_type: string (nullable = true)\n",
      " |    |-- Roof: string (nullable = true)\n",
      " |    |-- Rooms: string (nullable = true)\n",
      " |    |-- Toilet: string (nullable = true)\n",
      " |    |-- Floors: string (nullable = true)\n",
      " |    |-- Energy_label: string (nullable = true)\n",
      " |    |-- Position: string (nullable = true)\n",
      " |    |-- Garden: string (nullable = true)\n",
      " |    |-- Estimated_neighbourhood_price_per: string (nullable = true)\n",
      " |    |-- Availability: boolean (nullable = true)\n",
      " |    |-- event_time: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- Lot_size: string (nullable = true)\n",
      " |-- Living_space_size: string (nullable = true)\n",
      " |-- Build_year: string (nullable = true)\n",
      " |-- Build_type: string (nullable = true)\n",
      " |-- House_type: string (nullable = true)\n",
      " |-- Roof: string (nullable = true)\n",
      " |-- Rooms: string (nullable = true)\n",
      " |-- Toilet: string (nullable = true)\n",
      " |-- Floors: string (nullable = true)\n",
      " |-- Energy_label: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Garden: string (nullable = true)\n",
      " |-- Estimated_neighbourhood_price_per: string (nullable = true)\n",
      " |-- Availability: boolean (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 0eb3faca-5bae-442b-8078-ed70944e4053, runId = 0a37ae8f-f959-4875-8a00-5b244630f97a] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 98\u001B[0m\n\u001B[1;32m     94\u001B[0m query \u001B[38;5;241m=\u001B[39m top_10_prices_df\u001B[38;5;241m.\u001B[39mwriteStream\u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     95\u001B[0m                     \u001B[38;5;241m.\u001B[39mtrigger(processingTime \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2 seconds\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mforeachBatch(my_foreach_batch_function)\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 98\u001B[0m     \u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m    100\u001B[0m     query\u001B[38;5;241m.\u001B[39mstop()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = 0eb3faca-5bae-442b-8078-ed70944e4053, runId = 0a37ae8f-f959-4875-8a00-5b244630f97a] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition."
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, TimestampType\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Lab9_Ex3\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# We need to set the following configuration whenever we need to use GCS.\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"temp_degroup11\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Define the PySpark schema for the streaming data\n",
    "data_schema = StructType([\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True),\n",
    "    StructField(\"Lot_size\", StringType(), True),\n",
    "    StructField(\"Living_space_size\", StringType(), True),\n",
    "    StructField(\"Build_year\", StringType(), True),\n",
    "    StructField(\"Build_type\", StringType(), True),\n",
    "    StructField(\"House_type\", StringType(), True),\n",
    "    StructField(\"Roof\", StringType(), True),\n",
    "    StructField(\"Rooms\", StringType(), True),\n",
    "    StructField(\"Toilet\", StringType(), True),\n",
    "    StructField(\"Floors\", StringType(), True),\n",
    "    StructField(\"Energy_label\", StringType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Garden\", StringType(), True),\n",
    "    StructField(\"Estimated_neighbourhood_price_per\", StringType(), True),\n",
    "    StructField(\"Availability\", BooleanType(), True),\n",
    "    StructField(\"event_time\",TimestampType(), True),\n",
    "])\n",
    "\n",
    "#Loading the Cookie\n",
    "cookie_id_df = spark.read \\\n",
    "      .format(\"bigquery\") \\\n",
    "      .load(\" degroup11.group11dataset.cookie_ID_houses\")\n",
    "\n",
    "#Getting the mortgage threshold\n",
    "price_threshold = cookie_id_df.first()[\"possible_mortgage_amount\"]\n",
    "\n",
    "# Read the whole dataset as a batch\n",
    "kafkaStream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"subscribe\", \"mock\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "df = kafkaStream.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df1 = df.select(from_json(df.value, data_schema.simpleString()))\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "sdf = df1.select(col(\"from_json(value).*\"))\n",
    "\n",
    "sdf.printSchema()\n",
    "\n",
    "top_10_prices_df = sdf \\\n",
    "    .groupBy(window(col(\"event_time\"), \"10 seconds\"),\"Address\",\"City\", \"Price\",\"Availability\") \\\n",
    "    .agg(F.max(\"event_time\").alias(\"event_time\")) \\\n",
    "    .where((col(\"Price\") <= price_threshold) & (col(\"Availability\") == True)) \\\n",
    "    .orderBy(\"Price\", ascending=False)\\\n",
    "    .limit(500)\n",
    " \n",
    "top_10_prices_df = top_10_prices_df.dropDuplicates([\"Address\", \"Price\"])\n",
    "\n",
    "\n",
    "def my_foreach_batch_function(df, batch_id):\n",
    "    \n",
    "    df.show()\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', 'degroup11.group11dataset.house_pricing_kafka') \\\n",
    "      .mode(\"append\") \\\n",
    "      .save()\n",
    "\n",
    "query = top_10_prices_df.writeStream.outputMode(\"complete\") \\\n",
    "                    .trigger(processingTime = '2 seconds').foreachBatch(my_foreach_batch_function).start()\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stopped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94bde1aa-42ad-48b2-aa60-11ce41ed687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4f977-125f-4a01-b4c8-3458ad160123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
