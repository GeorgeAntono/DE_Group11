{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903ad5ab-57ec-49d8-bc97-ebba0a933187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"DataSourceSinkExample\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4aa0ef-9a51-40c5-b31c-0212480e9d73",
   "metadata": {},
   "source": [
    "## Get dataframes from google cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b490f3-55bc-42c7-ac81-1b9e636af35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Lot_size: string (nullable = true)\n",
      " |-- Living_space_size: string (nullable = true)\n",
      " |-- Build_year: string (nullable = true)\n",
      " |-- Build_type: string (nullable = true)\n",
      " |-- House_type: string (nullable = true)\n",
      " |-- Roof: string (nullable = true)\n",
      " |-- Rooms: string (nullable = true)\n",
      " |-- Toilet: string (nullable = true)\n",
      " |-- Floors: string (nullable = true)\n",
      " |-- Energy_label: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Garden: string (nullable = true)\n",
      " |-- Estimated_neighbourhood_price_per: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- has_spouse: string (nullable = true)\n",
      " |-- gross_salary: string (nullable = true)\n",
      " |-- has_student_loan: string (nullable = true)\n",
      " |-- student_loan_amount: string (nullable = true)\n",
      " |-- has_general_loan: string (nullable = true)\n",
      " |-- general_loan_amount: string (nullable = true)\n",
      " |-- has_alimony: string (nullable = true)\n",
      " |-- alimony_amount: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gross_salary: string (nullable = true)\n",
      " |-- has_student_loan: string (nullable = true)\n",
      " |-- student_loan_amount: string (nullable = true)\n",
      " |-- has_general_loan: string (nullable = true)\n",
      " |-- general_loan_amount: string (nullable = true)\n",
      " |-- has_alimony: string (nullable = true)\n",
      " |-- alimony_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#  Google Storage File Path\n",
    "gsc_file_path = 'gs://data_degroup11/house_pricing.csv'  #  use your gcp bucket name. Also upload sales.csv first\n",
    "gsc_file_path_2 = 'gs://data_degroup11/individuals.csv'\n",
    "gsc_file_path_3 = 'gs://data_degroup11/spouse.csv'\n",
    "# Create data frame\n",
    "df_house = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(gsc_file_path)\n",
    "df_house.printSchema()\n",
    "\n",
    "df_ind= spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(gsc_file_path_2)\n",
    "df_ind.printSchema()\n",
    "\n",
    "df_spouse = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(gsc_file_path_3)\n",
    "df_spouse.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7469d-da35-45c8-b8b4-7d9142cff5ff",
   "metadata": {},
   "source": [
    "# Preprocess the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048c22c3-d2e8-490e-a04c-bef10b73882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing values\n",
    "df_ind = df_ind.na.fill({'has_alimony': False})\n",
    "df_spouse = df_spouse.na.fill({'has_alimony': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc864a-5431-400a-8d21-85fcc97fd2a3",
   "metadata": {},
   "source": [
    "## Preproceses the individuals & Spouse dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad313803-cc22-4313-8c96-e4d5cc10184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names of df_ind:\n",
      "['ID', 'age', 'has_spouse', 'gross_salary', 'has_student_loan', 'student_loan_amount', 'has_general_loan', 'general_loan_amount', 'has_alimony', 'alimony_amount']\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- has_spouse: boolean (nullable = true)\n",
      " |-- spouse_ID: string (nullable = true)\n",
      " |-- gross_salary: string (nullable = true)\n",
      " |-- has_student_loan: string (nullable = true)\n",
      " |-- student_loan_amount: string (nullable = true)\n",
      " |-- has_general_loan: string (nullable = true)\n",
      " |-- general_loan_amount: string (nullable = true)\n",
      " |-- has_alimony: string (nullable = false)\n",
      " |-- alimony_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, lit, current_timestamp, when, rand\n",
    "\n",
    "# Print the column names of df_ind\n",
    "print(\"Column Names of df_ind:\")\n",
    "print(df_ind.columns)\n",
    "\n",
    "# Rename the 'ID' column to 'spouse_ID' in df_spouse\n",
    "df_spouse = df_spouse.withColumnRenamed('ID', 'spouse_ID')\n",
    "\n",
    "# Add a new column 'spouse_id' with the same values as 'ID'\n",
    "df_ind = df_ind.withColumn('spouse_ID', col('ID'))\n",
    "\n",
    "# Reorder the columns based on the desired configuration\n",
    "new_columns = ['ID', 'age', 'has_spouse', 'spouse_ID', 'gross_salary', 'has_student_loan', 'student_loan_amount', 'has_general_loan', 'general_loan_amount', 'has_alimony', 'alimony_amount']\n",
    "df_ind = df_ind.select(*new_columns)\n",
    "\n",
    "# Round up the values in the alimony_amount column and cast to integer\n",
    "df_ind = df_ind.withColumn(\"alimony_amount\", col(\"alimony_amount\").cast(\"int\"))\n",
    "df_ind = df_ind.withColumn(\"alimony_amount\", expr(\"ROUND(alimony_amount)\").cast(\"int\"))\n",
    "df_ind = df_ind.withColumn(\"has_spouse\", col(\"has_spouse\").cast(\"boolean\"))\n",
    "\n",
    "\n",
    "df_spouse = df_spouse.withColumn(\"alimony_amount\", col(\"alimony_amount\").cast(\"int\"))\n",
    "df_spouse = df_spouse.withColumn(\"alimony_amount\", expr(\"ROUND(alimony_amount)\").cast(\"int\"))\n",
    "\n",
    "df_ind.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31dc973-2860-4028-89ef-9ae6f78c9243",
   "metadata": {},
   "source": [
    "## Preprocess the housing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872681ff-608d-42b1-89bb-8c80e886601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5555\n",
      "there are  5519 rows in the dataframe\n",
      "now there are  5489 rows in the dataframe\n"
     ]
    }
   ],
   "source": [
    "# Add a new column 'Availability' with random True or False values\n",
    "df_house = df_house.withColumn('Availability', rand() < lit(0.5))\n",
    "\n",
    "# Clean the 'Price' column and convert it to integer\n",
    "df_house = df_house.withColumn('Price', expr(\"CAST(REGEXP_REPLACE(SUBSTR(Price, 3), '[^0-9]', '') AS INT)\"))\n",
    "\n",
    "print(df_house.count())\n",
    "print(\"there are \", df_house.distinct().count(), \"rows in the dataframe\")\n",
    "#print(df_house.select('Address', \"City\", \"Price\").distinct().count())\n",
    "\n",
    "df_house = df_house.dropDuplicates((\"Address\",\"City\", \"Price\"))\n",
    "print(\"now there are \", df_house.count(), \"rows in the dataframe\")\n",
    "\n",
    "# Define a window specification\n",
    "window_spec = Window().orderBy(F.monotonically_increasing_id())\n",
    "\n",
    "# Add a new column 'event_time' with a timestamp expression and a 5-second interval\n",
    "df_house = df_house.withColumn('event_time', current_timestamp() + F.expr(\"interval 5 seconds\") * F.row_number().over(window_spec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e48b809-6e01-4456-a24b-00b60828133b",
   "metadata": {},
   "source": [
    "## Save the dataframes to the google cloud bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          event_time|\n",
      "+--------------------+\n",
      "|2023-12-04 18:15:...|\n",
      "|2023-12-04 18:15:...|\n",
      "|2023-12-04 18:15:...|\n",
      "|2023-12-04 18:15:...|\n",
      "|2023-12-04 18:15:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:16:...|\n",
      "|2023-12-04 18:17:...|\n",
      "|2023-12-04 18:17:...|\n",
      "|2023-12-04 18:17:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+-------+\n",
      "|Availability|  Price|\n",
      "+------------+-------+\n",
      "|        true| 462500|\n",
      "|       false| 350000|\n",
      "|       false| 625000|\n",
      "|        true| 420000|\n",
      "|       false| 375000|\n",
      "|        true| 189000|\n",
      "|        true| 365000|\n",
      "|        true|1890000|\n",
      "|       false| 580000|\n",
      "|        true| 250000|\n",
      "|       false| 415000|\n",
      "|        true| 325000|\n",
      "|        true| 570000|\n",
      "|        true| 425000|\n",
      "|        true| 525000|\n",
      "|       false| 445000|\n",
      "|        true| 520000|\n",
      "|        true| 450000|\n",
      "|        true| 425000|\n",
      "|        true| 469000|\n",
      "+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf=df_house.select('event_time')\n",
    "newdf.show()\n",
    "\n",
    "\n",
    "newDf_house = df_house.select('Availability', 'Price')  # select columns\n",
    "\n",
    "newDf_house.show()\n",
    "\n",
    "# Specify the GCS path where you want to save the new DataFrames\n",
    "output_path_ind = 'gs://data_degroup11/individuals_updated2.csv'\n",
    "output_path_spouse = 'gs://data_degroup11/spouse_updated2.csv'\n",
    "output_path_house = 'gs://data_degroup11/house_pricing_updated2.csv'\n",
    "\n",
    "# Write the new DataFrames to GCS\n",
    "df_ind.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_path_ind)\n",
    "df_spouse.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_path_spouse)\n",
    "df_house.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_path_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3009a1a6d8c73d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
